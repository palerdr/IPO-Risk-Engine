{
  "permissions": {
    "allow": [
      "Bash(python scripts/smoke_test_bars.py:*)",
      "Bash(..venvScriptspython.exe:*)",
      "Bash(cmd /c \".venv\\\\Scripts\\\\python.exe scripts\\\\test_streets.py\")",
      "Bash(powershell:*)",
      "Bash(.venv/Scripts/pip.exe install:*)",
      "Bash(.venv/Scripts/python.exe:*)",
      "Bash(.venvScriptspython.exe scriptstest_river_features.py)",
      "Bash(ls -la \"c:\\\\Users\\\\jcena\\\\OneDrive\\\\Desktop\\\\Personal_Projects\\\\ipo-risk-engine\\\\scripts\"\" 2>/dev/null || echo \"scripts directory may not exist \")",
      "WebFetch(domain:stockanalysis.com)",
      "WebFetch(domain:www.iposcoop.com)",
      "Bash(python -m scripts.build_dataset:*)",
      "Bash(where:*)",
      "Bash(nvim:*)",
      "Bash(node --version:*)",
      "Bash(npm --version:*)",
      "Bash(python -m scripts.run_baselines:*)",
      "Bash(python:*)",
      "Bash(done)",
      "Bash(pip install:*)",
      "WebFetch(domain:site.warrington.ufl.edu)",
      "Bash(set:*)",
      "Bash(findstr:*)",
      "Bash(cmd /c \"cd /d c:\\\\Users\\\\jcena\\\\OneDrive\\\\Desktop\\\\Personal_Projects\\\\ipo-risk-engine && set PYTHONIOENCODING=utf-8 && python -m scripts.run_pilot\")",
      "Bash(python3:*)",
      "Bash(\"c:/Users/jcena/OneDrive/Desktop/Personal_Projects/ipo-risk-engine/_eval_tmp.py\" << 'PYEOF'\nimport polars as pl\nimport numpy as np\nfrom pathlib import Path\nimport sys\nsys.path.insert\\(0, 'src'\\)\n\nfrom ipo_risk_engine.policy.actions import \\(\n    min_false_safe_rate, feasibility_report,\n    assign_actions_by_rank, coverage_report, false_safe_rate,\n    should_calibrate,\n\\)\n\nds = Path\\('data/dataset'\\)\ntrain = pl.read_parquet\\(ds / 'train.parquet'\\)\nval = pl.read_parquet\\(ds / 'val.parquet'\\)\ntest = pl.read_parquet\\(ds / 'test.parquet'\\)\nall_df = pl.concat\\([train, val, test]\\)\n\nprint\\('=' * 70\\)\nprint\\('FULL DATASET EVALUATION -- RIVER POLICY'\\)\nprint\\('=' * 70\\)\nprint\\(f'Total: {all_df.height} rows \\({all_df.height // 3} symbols\\)'\\)\nprint\\(f'Train: {train.height}, Val: {val.height}, Test: {test.height}'\\)\nprint\\(f'Columns: {all_df.width}'\\)\nprint\\(\\)\n\n# 1. Coverage by era\nprint\\('=' * 60\\)\nprint\\('1. COVERAGE BY ERA'\\)\nprint\\('=' * 60\\)\nsym_era = all_df.filter\\(pl.col\\('street'\\) == 'FLOP'\\).with_columns\\(\n    pl.when\\(pl.col\\('ipo_date'\\) < pl.lit\\('2017-01-01'\\).str.to_date\\('%Y-%m-%d'\\)\\)\n    .then\\(pl.lit\\('2010-2016'\\)\\)\n    .when\\(pl.col\\('ipo_date'\\) < pl.lit\\('2022-01-01'\\).str.to_date\\('%Y-%m-%d'\\)\\)\n    .then\\(pl.lit\\('2017-2021'\\)\\)\n    .otherwise\\(pl.lit\\('2022-2025'\\)\\)\n    .alias\\('era'\\)\n\\).select\\(['symbol', 'era']\\)\n\nall_era = all_df.join\\(sym_era, on='symbol'\\)\n\nfor name, df in [\\('train', train\\), \\('val', val\\), \\('test', test\\)]:\n    era_df = df.filter\\(pl.col\\('street'\\) == 'RIVER'\\).join\\(sym_era, on='symbol'\\)\n    era_ct = era_df.group_by\\('era'\\).agg\\(pl.len\\(\\).alias\\('n'\\)\\).sort\\('era'\\)\n    vals = {r['era']: r['n'] for r in era_ct.iter_rows\\(named=True\\)}\n    total = sum\\(vals.values\\(\\)\\)\n    print\\(f'  {name:6s} RIVER: {vals}  \\(total={total}\\)'\\)\nprint\\(\\)\n\n# 2. Sample count by street\nprint\\('=' * 60\\)\nprint\\('2. SAMPLE COUNT BY STREET'\\)\nprint\\('=' * 60\\)\nfor name, df in [\\('train', train\\), \\('val', val\\), \\('test', test\\)]:\n    counts = df.group_by\\('street'\\).agg\\(pl.len\\(\\).alias\\('n'\\)\\).sort\\('street'\\)\n    row_str = '  '.join\\(f'{r[\"street\"]}={r[\"n\"]}' for r in counts.iter_rows\\(named=True\\)\\)\n    print\\(f'  {name:6s}: {row_str}'\\)\nprint\\(\\)\n\n# 3. Label coverage at RIVER\nprint\\('=' * 60\\)\nprint\\('3. RIVER LABEL COVERAGE'\\)\nprint\\('=' * 60\\)\nriver_all = all_df.filter\\(pl.col\\('street'\\) == 'RIVER'\\)\nfor col in ['forward_mdd_7d', 'forward_mdd_20d', 'risk_severity', 'adverse_20', 'severe_30']:\n    nn = river_all[col].drop_nulls\\(\\).len\\(\\)\n    print\\(f'  {col:25s}: {nn}/{river_all.height} \\({100*nn/river_all.height:.0f}%\\)'\\)\nprint\\(\\)\n\n# 4. Severe event counts\nprint\\('=' * 60\\)\nprint\\('4. SEVERE EVENT ANALYSIS \\(RIVER\\)'\\)\nprint\\('=' * 60\\)\nriver_labeled = river_all.drop_nulls\\(subset=['risk_severity']\\)\nn_labeled = river_labeled.height\nsev = river_labeled['risk_severity'].to_numpy\\(\\)\n\nfor thresh, label in [\\(0.10, '|MDD|>=10%'\\), \\(0.15, '|MDD|>=15%'\\), \\(0.20, 'adverse_20'\\), \\(0.30, 'severe_30'\\), \\(0.40, '|MDD|>=40%'\\)]:\n    ct = int\\(\\(sev >= thresh\\).sum\\(\\)\\)\n    print\\(f'  {label:15s}: {ct}/{n_labeled} \\({100*ct/n_labeled:.1f}%\\)'\\)\n\nprint\\(f'\\\\n  Per-split severe_30 counts:'\\)\nfor name, df in [\\('train', train\\), \\('val', val\\), \\('test', test\\)]:\n    rd = df.filter\\(pl.col\\('street'\\) == 'RIVER'\\).drop_nulls\\(subset=['severe_30']\\)\n    ct = int\\(rd['severe_30'].sum\\(\\)\\)\n    print\\(f'    {name}: {ct}/{rd.height} \\({100*ct/rd.height:.1f}%\\)'\\)\nprint\\(\\)\n\n# 5. Calibration gate\nprint\\('=' * 60\\)\nprint\\('5. CALIBRATION GATE'\\)\nprint\\('=' * 60\\)\nriver_train = train.filter\\(pl.col\\('street'\\) == 'RIVER'\\).drop_nulls\\(subset=['severe_30']\\)\ntrain_severe = river_train['severe_30'].to_numpy\\(\\).astype\\(float\\)\nn_severe_train = int\\(train_severe.sum\\(\\)\\)\ncal_ok = should_calibrate\\(train_severe\\)\nprint\\(f'  Train severe_30 events: {n_severe_train}'\\)\nprint\\(f'  Calibration gate \\(>=50\\): {\"PASS\" if cal_ok else \"FAIL\"}'\\)\nprint\\(\\)\n\n# 6. Feasibility check\nprint\\('=' * 60\\)\nprint\\('6. FEASIBILITY CHECK'\\)\nprint\\('=' * 60\\)\nfeas = feasibility_report\\(train_severe, max_false_safe_rate=0.10\\)\nprint\\(f'  p_severe \\(severe_30 base rate\\): {feas[\"p_severe\"]:.3f}'\\)\nprint\\(f'  max_feasible_sizeup \\(at FSR<=10%\\): {feas[\"max_feasible_sizeup\"]:.3f}'\\)\nfor c in [20, 30, 40, 50]:\n    mfsr = feas[f'min_fsr_at_{c}pct_sizeup']\n    ok = feas[f'feasible_at_{c}pct_sizeup']\n    status = 'FEASIBLE' if ok else 'INFEASIBLE'\n    print\\(f'  SIZE_UP={c}%: min_fsr={mfsr:.3f} -> {status}'\\)\nprint\\(\\)\n\n# 7. Oracle ranking policy\nprint\\('=' * 60\\)\nprint\\('7. ORACLE RANKING POLICY \\(RIVER only, severe_30 constraint\\)'\\)\nprint\\('=' * 60\\)\n\nriver_train_full = train.filter\\(pl.col\\('street'\\) == 'RIVER'\\).drop_nulls\\(subset=['risk_severity']\\)\nriver_val_full = val.filter\\(pl.col\\('street'\\) == 'RIVER'\\).drop_nulls\\(subset=['risk_severity']\\)\nriver_test_full = test.filter\\(pl.col\\('street'\\) == 'RIVER'\\).drop_nulls\\(subset=['risk_severity']\\)\n\nprint\\(f'  RIVER with labels: train={river_train_full.height}, val={river_val_full.height}, test={river_test_full.height}'\\)\n\nconfigs = [\n    \\('Q75/Q50 \\(default\\)', 0.75, 0.50\\),\n    \\('Q70/Q40', 0.70, 0.40\\),\n    \\('Q60/Q30', 0.60, 0.30\\),\n    \\('Q50/Q25', 0.50, 0.25\\),\n]\n\nfor config_name, fold_q, sbet_q in configs:\n    print\\(f'\\\\n  --- {config_name} ---'\\)\n    \n    train_scores = river_train_full['risk_severity'].to_numpy\\(\\)\n    fold_thresh = float\\(np.percentile\\(train_scores, fold_q * 100\\)\\)\n    sbet_thresh = float\\(np.percentile\\(train_scores, sbet_q * 100\\)\\)\n    \n    for split_name, split_df in [\\('train', river_train_full\\), \\('val', river_val_full\\), \\('test', river_test_full\\)]:\n        scores = split_df['risk_severity'].to_numpy\\(\\)\n        n = len\\(scores\\)\n        \n        actions = []\n        for s in scores:\n            if s >= fold_thresh:\n                actions.append\\('FOLD'\\)\n            elif s >= sbet_thresh:\n                actions.append\\('SMALL_BET'\\)\n            else:\n                actions.append\\('SIZE_UP'\\)\n        \n        fold_ct = actions.count\\('FOLD'\\)\n        sbet_ct = actions.count\\('SMALL_BET'\\)\n        sizeup_ct = actions.count\\('SIZE_UP'\\)\n        \n        severe = split_df['severe_30'].to_numpy\\(\\).astype\\(bool\\)\n        severe_ct = int\\(severe.sum\\(\\)\\)\n        if severe_ct > 0:\n            severe_sizeup = sum\\(1 for a, s in zip\\(actions, severe\\) if s and a == 'SIZE_UP'\\)\n            fsr = severe_sizeup / severe_ct\n        else:\n            fsr = 0.0\n            severe_sizeup = 0\n        \n        adverse = split_df['adverse_20'].to_numpy\\(\\).astype\\(bool\\)\n        adverse_ct = int\\(adverse.sum\\(\\)\\)\n        if adverse_ct > 0:\n            adverse_fold = sum\\(1 for a, s in zip\\(actions, adverse\\) if s and a == 'FOLD'\\)\n            adr = adverse_fold / adverse_ct\n        else:\n            adr = 0.0\n        \n        fsr_status = 'PASS' if fsr <= 0.10 else 'FAIL'\n        print\\(f'  {split_name:6s} N={n:4d}  FOLD={fold_ct}\\({100*fold_ct/n:.0f}%\\) SBET={sbet_ct}\\({100*sbet_ct/n:.0f}%\\) SIZE_UP={sizeup_ct}\\({100*sizeup_ct/n:.0f}%\\)'\\)\n        print\\(f'         FSR\\(sev30\\)={fsr:.1%} [{fsr_status}] \\({severe_sizeup}/{severe_ct}\\)  ADR\\(adv20\\)={adr:.1%}'\\)\n\nprint\\(\\)\nprint\\('=' * 70\\)\nprint\\('SUMMARY'\\)\nprint\\('=' * 70\\)\nprint\\(f'Full dataset: {all_df.height} snapshots, {all_df.height//3} symbols'\\)\nprint\\(f'RIVER gate: PASS \\(95.4%\\)'\\)\nprint\\(f'Calibration gate: {\"PASS\" if cal_ok else \"FAIL\"} \\({n_severe_train} severe_30 in train, need 50\\)'\\)\nprint\\(f'Feasibility: max SIZE_UP = {feas[\"max_feasible_sizeup\"]:.0%} at 10% FSR constraint'\\)\nprint\\(f'SIP failures: 32 symbols \\(all IPOs after ~Sep 2025\\)'\\)\nPYEOF)",
      "Bash(iconv:*)",
      "Bash(file:*)",
      "Bash(ls:*)"
    ]
  },
  "outputStyle": "Learning"
}
